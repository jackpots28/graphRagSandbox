{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6aa2b53283cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Install some packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j import exceptions\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import secrets\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ead3c53095ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup container image env vars for auth - neo4j graph db\n",
    "\n",
    "# GEN rand password for temp graphDB\n",
    "one_time_password: str = secrets.token_urlsafe(23)\n",
    "\n",
    "os.environ['NEO4J_DRIV_PORT'] = '7687'\n",
    "os.environ['NEO4J_HTTP_PORT'] = '7474'\n",
    "os.environ['NEO4J_USERNAME'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = one_time_password\n",
    "\n",
    "!echo ${NEO4J_DRIV_PORT}\n",
    "!echo ${NEO4J_HTTP_PORT}\n",
    "!echo ${NEO4J_USERNAME}\n",
    "!echo ${NEO4J_PASSWORD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde8712c3007213",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop existing graph db completely (rm container) and rebuild from fresh\n",
    "\n",
    "# Pull latest image if none exist - continue without pulling if already present\n",
    "!docker images neo4j:latest | awk 'NR>1{print $1}' | if [[ $_ != \"neo4j:latest\" ]]; then docker pull neo4j:latest; else printf \"NEO4J image present.\\n\"; fi\n",
    "\n",
    "!docker ps -a | grep -iE 'neo4j|tini -g' | awk '{print $1}' | xargs -I{} docker rm {} -f\n",
    "!docker ps -a | grep -ie neo4j\n",
    "!docker run -d --restart always --publish=${NEO4J_HTTP_PORT}:${NEO4J_HTTP_PORT} --publish=${NEO4J_DRIV_PORT}:${NEO4J_DRIV_PORT} --env NEO4J_AUTH=${NEO4J_USERNAME}/${NEO4J_PASSWORD} neo4j:latest\n",
    "!docker ps -a | grep -ie neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5a76de1c4c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the embeddings model: msmarco-MiniLM-L-12-v3 or all-mpnet-base-v2 - using sentence-transformers locally\n",
    "# Using a dict to store multiple model references - easier to keep track of things I've tried and whats present\n",
    "ollama_model_library = {\n",
    "    \"smollm\":\"smollm:1.7b\",\n",
    "    \"llama3.1\":\"llama3.1:8b\",\n",
    "    \"all-minilm\":\"all-minilm:l6-v2\",\n",
    "    \"llama3\":\"llama3:latest\",\n",
    "    \"mxbai-embed-large\":\"mxbai-embed-large:latest\",\n",
    "    \"codellama\":\"codellama:13b\"\n",
    "}\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=ollama_model_library['mxbai-embed-large'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62ea44da5209a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original test documents\n",
    "documents_dictionary_struct = {\n",
    "    \"smartbear\": Path(\"/Volumes/stuff/graphRagSandbox/assets/SmartBear_TOU-10FEB2023.docx\"),\n",
    "    \"shrekmovie\": Path(\"/Volumes/stuff/general_playground/assets/the_entire_shrek_script.txt\"),\n",
    "    \"dantesinferno\": Path(\"/Volumes/stuff/general_playground/assets/dantes_inferno_all_chp.txt\")\n",
    "}\n",
    "\n",
    "# loader = TextLoader(smart_bear_contract)\n",
    "docx_loader = Docx2txtLoader(documents_dictionary_struct[\"smartbear\"])\n",
    "documents = docx_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ade4020474afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a dataclass incase I reference the same variables for splitter construction - one-stop shop for changing vars during testing\n",
    "@dataclass\n",
    "class RecTxtDataClass:\n",
    "    c_size: int = 120\n",
    "    c_overlap: int = 20\n",
    "    c_separators: List = field(default_factory=lambda: [\"\\n\\n\", \"\\n\", \".\"])\n",
    "    c_len_fun: len = lambda x: len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1ca60b67ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "RecTxtObj = RecTxtDataClass()\n",
    "\n",
    "# text_splitter = NLTKTextSplitter(chunk_size=1500, chunk_overlap=20, separator=\". \")\n",
    "# I used a recursive splitter since I wanted to break on paragraph, complete sentence, and single words (but not single chars) - this worked well for legal docs\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=RecTxtObj.c_size, \n",
    "                                                    chunk_overlap=RecTxtObj.c_overlap, \n",
    "                                                    separators=RecTxtObj.c_separators, \n",
    "                                                    length_function=RecTxtObj.c_len_fun)\n",
    "\n",
    "# nltk_docs = text_splitter.split_documents(documents)\n",
    "rec_docs = recursive_splitter.split_documents(documents)\n",
    "rec_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a7d4546ac5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name_only = documents_dictionary_struct[\"smartbear\"].with_suffix(\"\").name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32237dfcf4b43ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = f'neo4j://localhost:{os.environ['NEO4J_DRIV_PORT']}'\n",
    "USERNAME = os.environ['NEO4J_USERNAME']\n",
    "PASSWORD = os.environ['NEO4J_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce50230aedfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))\n",
    "\n",
    "db = Neo4jVector.from_documents(\n",
    "    rec_docs, ollama_emb, url=URI, username=USERNAME, password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65df431ddad02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_db_index_create: str = f\"CREATE FULLTEXT INDEX text_index IF NOT EXISTS FOR (n:Chunk) ON EACH[n.text]\"\n",
    "graph_db_fulltext_label_query: str = f\"CALL db.index.fulltext.queryNodes(\\\"text_index\\\", \\\".\\\") YIELD node RETURN node.id\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    # session.run(\"CREATE FULLTEXT INDEX text_index IF NOT EXISTS FOR (n:Chunk) ON EACH[n.text]\")\n",
    "    try:\n",
    "        result = session.run(graph_db_index_create)\n",
    "    except exceptions.ClientError as e:\n",
    "        print(f\"FULLTEXT INDEX - Not Present\")\n",
    "        print(e)\n",
    "    finally:\n",
    "        session.run(graph_db_fulltext_label_query)\n",
    "        print(f\"text_index - Created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3763197021b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"vector\"\n",
    "keyword_index_name = \"text_index\"\n",
    "search_type = \"hybrid\"\n",
    "\n",
    "# Look at this article in streamlining the graphDB construction and index ingestion into simpler method:\n",
    "# https://medium.com/neo4j/using-langchain-in-combination-with-neo4j-to-process-youtube-playlists-and-perform-q-a-flow-5d245d51a735\n",
    "store = Neo4jVector.from_existing_index(\n",
    "    ollama_emb,\n",
    "    url=URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    index_name=index_name,\n",
    "    keyword_index_name=keyword_index_name,\n",
    "    search_type=search_type,\n",
    ")\n",
    "\n",
    "retriever = store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c400fc7cf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt worked well for submitting lists of lookup phrases and receiving lists of found verified matches\n",
    "prompt_template = \"\"\"You will be give a list of phrases to search for within the context provided. Strictly follow these outlined rules when producing answers:\n",
    "1. Only answer with information found within the context provided.\n",
    "2. If you don't know the answer, don't try to make up an answer. Just answer with NONE.\n",
    "3. If you find the answer, only respond with a list of complete sentences that pertain to the provided phrases in the following format: [sentence1, sentence2, ...]\n",
    "4. If you do not find the answer at first, attempt again but only once.\n",
    "5. Do not append newlines to end of response.\n",
    "\n",
    "This document pertains to: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "{summaries}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\", \"summaries\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563bd02fc1fd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "context: str = doc_name_only\n",
    "# question: str = input(f\"What is your question about {context}\")\n",
    "\n",
    "raw_question_list: list = [\n",
    "    'customer name apart of the contract',\n",
    "    'effective date of document',\n",
    "    'required notice time before non-renewal',\n",
    "    'what countries are excluded due to sanctions',\n",
    "    'who is not liable for delay of duties',\n",
    "    'how long is given to not pay before suspense',\n",
    "    'time for notice of non-renewal',\n",
    "    'how much time is given from invoice date to pay fees',\n",
    "    'any phrase present that discusses entity purchasing',\n",
    "    'paragraph that outline acknowledgments for binding a user to agreement of conditions',\n",
    "]\n",
    "\n",
    "sanitized_question_list = [item for item in raw_question_list if item != \"\"]\n",
    "\n",
    "formatted_question_list: str = \"[\"\n",
    "for iter_n in range(len(sanitized_question_list)):\n",
    "    if iter_n < len(sanitized_question_list)-1:\n",
    "        formatted_question_list += str(sanitized_question_list[iter_n])+\", \"\n",
    "    else:\n",
    "        formatted_question_list += str(sanitized_question_list[iter_n])+\"]\"\n",
    "\n",
    "# formatted_question_list: str = f\"[{raw_question_list[0]}, {raw_question_list[1]}, {raw_question_list[2]}]\"\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", \n",
    "                                  input_key=\"question\", \n",
    "                                  output_key='answer', \n",
    "                                  return_messages=True,\n",
    "                                  )\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:latest\", \n",
    "                 temperature=0.2,\n",
    "                 format=\"json\",\n",
    "                 )\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs={ \"prompt\": PROMPT },\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "response = ast.literal_eval(chain.invoke({'context': context, 'question': formatted_question_list})['answer'])\n",
    "\n",
    "# print(f\"Answer list: {response['answer']}\")\n",
    "for question, answer in response.items():\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804a3394de6fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
